import json
from flask import Flask, request, jsonify
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
from flask_cors import CORS

app = Flask(__name__)
CORS(app, resources={r"/compare": {"origins": "*"}})

# T·∫£i m√¥ h√¨nh AI m·∫°nh h∆°n
try:
    model_name = "EleutherAI/gpt-neo-1.3B"  # M√¥ h√¨nh m·∫°nh h∆°n GPT-2
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = pipeline("text-generation", model=model_name)
    max_length = tokenizer.model_max_length  # L·∫•y ƒë·ªô d√†i t·ªëi ƒëa c·ªßa m√¥ h√¨nh
    print(f"‚úÖ M√¥ h√¨nh AI ƒë√£ s·∫µn s√†ng! ƒê·ªô d√†i prompt t·ªëi ƒëa: {max_length}")
except Exception as e:
    print(f"‚ùå L·ªói khi t·∫£i m√¥ h√¨nh: {e}")
    model = None
    max_length = 512  # Gi√° tr·ªã m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c

@app.route('/compare', methods=['POST'])
def compare_laptops():
    if model is None:
        return jsonify({"message": "L·ªói: M√¥ h√¨nh AI ch∆∞a s·∫µn s√†ng"}), 500

    try:
        # ƒê·ªçc d·ªØ li·ªáu ƒë·∫ßu v√†o t·ª´ request
        data = request.get_json()
        print("üì• D·ªØ li·ªáu JSON nh·∫≠n ƒë∆∞·ª£c:", request.get_data(as_text=True))

        # X·ª≠ l√Ω l·ªói JSON ƒë·∫ßu v√†o
        if isinstance(data, str):
            data = json.loads(data)

        if not isinstance(data, dict):
            return jsonify({"message": "D·ªØ li·ªáu ƒë·∫ßu v√†o kh√¥ng h·ª£p l·ªá"}), 400

        # Ki·ªÉm tra th√¥ng tin laptop
        laptop1 = data.get('laptop1', {})
        laptop2 = data.get('laptop2', {})

        if not isinstance(laptop1, dict) or not isinstance(laptop2, dict):
            return jsonify({"message": "L·ªói: D·ªØ li·ªáu laptop kh√¥ng h·ª£p l·ªá"}), 400

        # Lo·∫°i b·ªè tr∆∞·ªùng description n·∫øu c√≥
        laptop1.pop("description", None)
        laptop2.pop("description", None)

        # L·∫•y th√¥ng tin ch√≠nh c·ªßa laptop
        laptop1_name = laptop1.get('name', 'Laptop 1')
        laptop2_name = laptop2.get('name', 'Laptop 2')
        cpu1 = laptop1.get('cpu', 'Kh√¥ng r√µ')
        cpu2 = laptop2.get('cpu', 'Kh√¥ng r√µ')
        gpu1 = laptop1.get('gpu', 'Kh√¥ng r√µ')
        gpu2 = laptop2.get('gpu', 'Kh√¥ng r√µ')
        ram1 = laptop1.get('ram', 'Kh√¥ng r√µ')
        ram2 = laptop2.get('ram', 'Kh√¥ng r√µ')
        price1 = laptop1.get('price', 'Kh√¥ng r√µ')
        price2 = laptop2.get('price', 'Kh√¥ng r√µ')

        # X√¢y d·ª±ng prompt t·ªët h∆°n
        prompt = (
            f"So s√°nh hi·ªáu nƒÉng gi·ªØa {laptop1_name} v√† {laptop2_name}. "
            f"{laptop1_name} c√≥ CPU {cpu1}, GPU {gpu1}, RAM {ram1}, gi√° {price1} VND. "
            f"{laptop2_name} c√≥ CPU {cpu2}, GPU {gpu2}, RAM {ram2}, gi√° {price2} VND. "
            "H√£y nh·∫≠n x√©t v·ªÅ hi·ªáu nƒÉng t·ªïng th·ªÉ, ∆∞u ƒëi·ªÉm v√† nh∆∞·ª£c ƒëi·ªÉm c·ªßa t·ª´ng laptop. "
            "ƒê∆∞a ra g·ª£i √Ω n√™n mua laptop n√†o d·ª±a tr√™n nhu c·∫ßu s·ª≠ d·ª•ng, kh√¥ng nh·∫Øc l·∫°i th√¥ng s·ªë k·ªπ thu·∫≠t."
        )
        prompt = prompt[:max_length]  # Gi·ªõi h·∫°n ƒë·ªô d√†i prompt theo m√¥ h√¨nh

        # G·ªçi AI x·ª≠ l√Ω
        response_list = model(prompt, max_new_tokens=50, num_return_sequences=1)

        if not response_list:
            return jsonify({"message": "L·ªói: Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ AI"}), 500

        response = response_list[0].get('generated_text', 'Kh√¥ng c√≥ ph·∫£n h·ªìi t·ª´ AI')

        return jsonify({"message": response})

    except json.JSONDecodeError:
        return jsonify({"message": "L·ªói: Kh√¥ng th·ªÉ gi·∫£i m√£ JSON"}), 400
    except Exception as e:
        print(f"‚ùå L·ªói khi x·ª≠ l√Ω y√™u c·∫ßu: {e}")
        return jsonify({"message": "L·ªói khi x·ª≠ l√Ω y√™u c·∫ßu AI"}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)
